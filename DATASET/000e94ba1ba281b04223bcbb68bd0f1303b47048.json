{
    "paper_id": "000e94ba1ba281b04223bcbb68bd0f1303b47048",
    "metadata": {
        "title": "DEEP CO-SUPERVISION AND ATTENTION FUSION STRATEGY FOR AUTOMATIC COVID-19 LUNG INFECTION SEGMENTATION ON CT IMAGES",
        "authors": [
            {
                "first": "Haigen",
                "middle": [],
                "last": "Hu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Zhejiang University of Technology",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Qiu",
                "middle": [],
                "last": "Guan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Zhejiang University of Technology",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Xiaoxin",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Zhejiang University of Technology",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Qianwei",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Zhejiang University of Technology",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Su",
                "middle": [],
                "last": "Ruan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Rouen Normandy",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Due to the irregular shapes,various sizes and indistinguishable boundaries between the normal and infected tissues, it is still a challenging task to accurately segment the infected lesions of COVID-19 on CT images. In this paper, a novel segmentation scheme is proposed for the infections of COVID-19 by enhancing supervised information and fusing multi-scale feature maps of different levels based on the encoder-decoder architecture. To this end, a deep collaborative supervision (Co-supervision) scheme is proposed to guide the network learning the features of edges and semantics. More specifically, an Edge Supervised Module (ESM) is firstly designed to highlight low-level boundary features by incorporating the edge supervised information into the initial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised Module (ASSM) is proposed to strengthen high-level semantic information by integrating mask supervised information into the later stage. Then an Attention Fusion Module (AFM) is developed to fuse multiple scale feature maps of different levels by using an attention mechanism to reduce the semantic gaps between high-level and low-level feature maps. Finally, the effectiveness of the proposed scheme is demonstrated on four various COVID-19 CT datasets. The results show that the proposed three modules are all promising. Based on the baseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase Dice metric by 1.12%, 1.95%,1.63% in our dataset, while the integration by incorporating three models together can rise 3.97%. Compared with the existing approaches in various datasets, the proposed method can obtain better segmentation performance in some main metrics, and can achieve the best generalization and comprehensive performance.The code is publicly available at https://github.com/HuHaigen/COVID-19-Lung-Infection-Segentation or https://github.com/slz674763180/COVID19. The package includes the proposed three modules and joint loss function for reproducibility purposes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "and segment COVID-19 lesion from chest CT images. Oulefki et al. [10] presented the utility of an automated tool of segmentation and measurement for COVID-19 lung Infection using chest CT imagery. Owing to the fact that lung infected region segmentation is a necessary initial step for lung image analysis, some image segmentation algorithms are also proposed for some specific application scenarios. For instance, an improved Inf-Net was proposed to segment the infection area of the novel coronavirus, and a semi-supervised training method is put forward to solve insufficient amount of labeled CT and improve the segmentation performance [6] . Currently, most of the methods are based on detection and classification tasks, but not much on the semantic segmentation of infection on CT slices [4] , so that the assessment and staging COVID-19 infection are greatly limited. Therefore, according to CT imaging characteristics, it is necessary to propose some segmentation methods for the infection regions of COVID-19, so that we can further achieve quantitative analysis of the lesions.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 69,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 641,
                    "end": 644,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 795,
                    "end": 798,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "However, it is a still challenging task to accurately segment the infected lesions of COVID-19 on CT images owing to the following facts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "1. The infections have irregular boundary, different sizes and shapes from slice to slice on CT images (shown in Figure 1(a) ). It would easily lead to missing some small ground-glass lesions or generating excessive over-segmentation for the infections on CT images.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 113,
                    "end": 124,
                    "text": "Figure 1(a)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "2. There seems to be no discernible difference between infections and normal tissues (shown in Figure 1 (b)). It is unaffected for the detection or classification, but it can decrease segmentation accuracy and quantified quality.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 95,
                    "end": 103,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "3. The existing semantic segmentation approaches like the encoder-decoder structure exist a \"semantic gap\" between low-level visual features and high-level semantic concepts, which greatly limits the efficiency of semantic segmentation. To address these issues, a novel segmentation scheme is proposed for the infections of COVID-19 based on the encoder-decoder architecture [11] in this paper, and the proposed scheme can collaboratively enhance supervised information of different levels and fuse different scale feature maps. For the proposed deep collaborative supervision scheme, we propose an Auxiliary Semantic Supervised Module (ASSM) and an Edge Supervised Module (ESM) to guide the network learning the features of edges and semantics in the encoding stage, respectively. As for multi-scale feature maps, an Attention Fusion Module (AFM), following with the decoding stage, is proposed to reduce the semantic gaps between high-level and low-level feature maps. The proposed attention fusion strategy can take full advantage of different scale context information. Finally, a series of experiments are conducted on the COVID-19 dataset to verify the effectiveness of the proposed scheme. The results show that our method can obtain better performance for the segmentation of COVID-19 infections than the existing approaches. The main contributions of this paper are listed as follows.",
            "cite_spans": [
                {
                    "start": 375,
                    "end": 379,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 An ESM is put forward to highlight low-level boundary features. The edge supervised information is incorporated into the initial stage of down-sampling, as the proposed edge supervised loss function allows to capture rich spatial information in various scales.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 An ASSM i s proposed to enhance high-level semantics from feature maps with different scales. The mask supervised information is introduced into the later stage of down-sampling, thanks to the corresponding auxiliary semantic loss function that is defined to explore sufficient semantic information from various scale infections on COVID-19 CT images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 An AFM is developed to fuse various scale feature maps from the up-sampling stage. An attention mechanism is utilized to reduce the semantic gaps between high-level and low-level feature maps, so as to strengthen and supplement the lost detailed information in high-level representations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 A joint loss function is constructed by combining the edge supervised loss, auxiliary semantic supervised loss and fusion loss. It can guide the network achieving a deep collaborative supervision on edges and semantics, and prompting the fusion efficiency on multiple scale feature maps from different levels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "This paper is organized as follows. Section 2 introduces the related works. Section 3 describes details about the proposed methods, including Edge Supervised Module (ESM), Auxiliary Semantic Supervised Module (ASSM) and Attention Fusion Module (AFM). Section 4 presents experiments, results and discussions, and Section 5 concludes this work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In this section, we provide a short review of previous studies on network models, edge supervision, multi-scale object recognition, and attention mechanism.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related works"
        },
        {
            "text": "Deep network models are a kind of hierarchical feature learning methods by learning multiple levels of representation to model complex relationships among data, and higher-level features and concepts are thus defined in terms of lower-level ones, and such a hierarchy of features is called a deep architecture [12] . Usually, the first layers will learn the low level features like intensity, color, lines, dots and curves, then the more the layers approach the output layer, the more the layers will learn the high level features like objects and shapes in a feature extracting pipeline. For example, from AlexNet [13] , VGG [14] to ResNet [15] , the ability of feature extraction is becoming more and more powerful with the deepening of the network depth. Accordingly, the deeper networks can provide a powerful feature extraction ability for semantic segmentation tasks, and can greatly improve segmentation accuracy.",
            "cite_spans": [
                {
                    "start": 310,
                    "end": 314,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 615,
                    "end": 619,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 626,
                    "end": 630,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 641,
                    "end": 645,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Network Models"
        },
        {
            "text": "Since FCN [16] is proposed, other semantic segmentation networks attempt to improve this architecture by adding new modules to solve the problems regarding the lack of spatial and contextual information. For example, U-Net [11] is greatly improved only by adding the skip connection based on FCN. PSPNet [17] employs pyramid pooling module to explore the global context information, and it can improve the accuracy of target segmentation at different scales. Besides, DeepLabV3+ [18] combines the advantages of Spatial Pyramid Pooling (SPP) module and encoder-decoder structure, and further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling (ASPP) and decoder modules. PSANet [19] can capture pixel level relationship and relative position information in spatial dimension through convolution layer. In addition, EncNet [21] also introduced a channel attention mechanism to capture the global context. Although many advanced network structures have been emerged for semantic segmentation tasks, U-Net and its derivatives are still the most popular architecture and have been widely applied in the medical imaging community [27, 37] . However, despite their outstanding overall performance in segmenting medical images, the U-Net-based architecture seems to be lacking in certain aspects. For example, although the high-level feature map can be optimized through the concatenation the feature maps of the low-level layers and the high-level layer by using skipping connection, it is still very difficult to reduce the semantic gap between low-level visual features and high-level semantic features. Thus, we select ResUNet as the backbone to attempt to exploit a novel segmentation architecture for the COVID-19 segmentation task in this work.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 223,
                    "end": 227,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 479,
                    "end": 483,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 738,
                    "end": 742,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 882,
                    "end": 886,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "text": "[27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1190,
                    "end": 1193,
                    "text": "37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Network Models"
        },
        {
            "text": "Edge information, as an important image feature, is drawing more and more attention in deep learning community owing to the fact that edge information is conducive to the extraction of object contour in segmentation tasks. For example, explicit edge-attention are utilized to model the boundaries and enhance the representations in [6] . Wu et al. [22] proposed a novel edge aware salient object detection method, and it passes messages between two tasks in two directions, and refines multi-level edge and segmentation features. ET-Net [23] integrates edge detection and object segmentation into a deep learning network, and the edge attention representation is embedded to supervise the segmentation prediction. Normally, edge information can provide useful fine-grained constraints to guide feature extraction in semantic segmentation tasks. However, high-level feature maps have little edge information, while low-level layers contain richer object boundaries.",
            "cite_spans": [
                {
                    "start": 332,
                    "end": 335,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 348,
                    "end": 352,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 537,
                    "end": 541,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Edge Supervision and Multi-scale Object Recognition"
        },
        {
            "text": "For the multi-scale object recognition problem, it is common practice to exploit multiple levels of coarse and fine-grained semantic features by adopting different network structures in computer vision. For example, the operations of convolution and pooling on the original image is used to obtain feature maps of different sizes, and it is similar to constructing pyramids in the feature space of images. Feature Pyramid Networks (FPN) [24] is one of the most typical examples, and it adopts a top-down architecture with lateral connections for building high-level semantic feature maps at all scales. It has been demonstrated a significant improvement as a generic feature extractor in detection tasks, and has been widely applied in different detection architectures, such as Faster R-CNN [25] and Mask R-CNN [26] .",
            "cite_spans": [
                {
                    "start": 437,
                    "end": 441,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 792,
                    "end": 796,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 812,
                    "end": 816,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Edge Supervision and Multi-scale Object Recognition"
        },
        {
            "text": "It is widely known that the low-level feature maps pay more attention to detail information, while the high-level lay much attention to semantic information. More specifically, the encoded pathway is mainly used for feature extraction, and there are hierarchy and gradation for various feature. Because the spatial resolution and the semantics can be decreased and strengthened along with the deepening of down-sampling, respectively. Significantly, FPN [24] and U-Net [11] both adopt encoder-decoder architecture, but they are respectively applied in object detection and semantic segmentation. The main difference is that there are multiple prediction layers for various scale features in FPN [24] . Inspired by this, we attempt to exploit sufficient multi-scale context information from different levels of the encoder in this work. Low level detailed feature maps can exploit rich spatial information, and they could strengthen the boundaries of the infected regions; while high-level semantic feature maps can endow position information, and they could locate the infected regions.",
            "cite_spans": [
                {
                    "start": 454,
                    "end": 458,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 469,
                    "end": 473,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 695,
                    "end": 699,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Edge Supervision and Multi-scale Object Recognition"
        },
        {
            "text": "Attention can be regarded as a mechanism, and it emphasizes the features that need attention through the context of feature maps. Normally, an attention mechanism is used to highlight the important context in the channel-wise or space-wise [7, 8] , while suppressing the context information irrelevant to the content. For example, Fu et al. [28] proposed Dual Attention Network (DAN), and two attention modules were introduced to capture the spatial dependence between any two positions in the feature maps. A similar self-attention mechanism was used to capture the channel dependence between any two channels, and the weighted sum of all channel was utilized to update each channel. Huang et al. [29] proposed Criss-Cross Net (CCNet) to capture this important information in a more effective way, specifically, for each pixel, CCNet can obtain the context information on its crisscross path through a Criss-Cross attention module. Non-local operations, proposed by Wang et al. [30] , can directly capture remote dependencies by calculating the interaction between any two locations. Besides, an attention mechanism is also used to aggregate different levels of features to bridge the semantic gaps between low-level features and high-level semantics. For example, Li et al. [31] proposed Gated Fully Fusion(GFF) to fully fuse multi-level feature maps controlled by learned gate maps, and the novel module can bridge the gap between high resolution with low semantics and low resolution with high semantics. Inspired by this, we adopt an attention mechanism to fuse various level feature maps, and the proposed AFM can reduce the semantic gaps between high-level and low-level feature maps, so as to strengthen and supplement the missing detailed information in high-level representations.",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 243,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 244,
                    "end": 246,
                    "text": "8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 341,
                    "end": 345,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 698,
                    "end": 702,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 979,
                    "end": 983,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Attention Mechanism"
        },
        {
            "text": "In this section, we first present the proposed network architecture. Then we introduce in details the proposed three modules: ESM, ASSM and AFM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "As mentioned above, U-Net [11] and FPN [24] both have a similar encoder-decoder structure for multi-scale object vision tasks, consisting of a contracting path to capture context and a symmetric expanding path that enables precise localization. While U-Net [11] creates a path for information propagation allowing signals propagate between low and high levels by copying low level features to the corresponding high levels. Despite achieving good segmentation performance in U-Net and its variations, however, the edge information and channels would decrease and increase along with down-sampling of the contracting path, respectively. Both cases can lead to effective information missing, thereby not exploring sufficient information from full scales so as to suffer segmentation performance degradation. While FPN [24] can overcome these drawbacks to retain multi-scale contextual information by using multiple prediction layers: one for each up-sampling layer. Based on this idea, we propose a novel segmentation scheme for the infections of COVID-19. Figure 2 illustrates the proposed network architecture. Firstly, we collaboratively enhance the supervised information by introducing edge and semantic information into the encoding stage. Note that the initial stages are used for the edge supervision, while the later stages for the semantic supervision. They occupy the whole down-sampling together, more precisely, the sum of the low-level and high-level layers is equal to the total layers of the encoder. Especially, low-level feature maps from shallow layers are with high resolution, but with limited semantics, whereas high-level feature maps from deep layers have low spatial resolution without detailed information (like object boundaries). When various levels are selected to enhance the supervised information, there is a trade-off between edge supervision and semantic supervision, thus we call it \"collaborative supervision\" (\"Co-supervision\"). Then we fuse multi-scale feature maps of different levels from the decoding stage in an encoder-decoder framework (like U-Net). Considering the fact that low level detailed feature maps have high resolution and can capture rich spatial information like object boundaries, we design an ESM to highlight low-level boundary features by incorporating the edge supervised information into the initial stage (like S 1 and S 2 in Figure 2 ) of down-sampling in the encoder. While high-level semantic feature maps embody position information like object concepts, thus we present an ASSM to strengthen high-level semantic information by integrating object mask supervised information into the later stage (like S 3 \u223c S 5 in Figure 2 ). Finally, the obtained various scale feature maps from the up-sampling stage are fused by adopting an attention mechanism to achieve good segmentation performance for infections of COVID-19.",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 30,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 39,
                    "end": 43,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 257,
                    "end": 261,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 816,
                    "end": 820,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [
                {
                    "start": 1055,
                    "end": 1063,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 2387,
                    "end": 2395,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 2680,
                    "end": 2688,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Proposed Network Architecture"
        },
        {
            "text": "Many studies [22, 23] show that the edge information can provide effective constraints to the feature extraction in the segmentation task. To supplement the missing edge information along with down-sampling, we propose ESM to further highlight the object boundary features in the low-level layers. Because feature maps of low level from shallow layers are with high resolution and detailed information (including edge information), and these detailed information are easily lost during the initial stage of the down-sampling process, the proposed ESM can capture more detailed information like object boundaries. Specifically, we can guide the network to extract edge features from the initial stages like S 1 and S 2 (shown in Figure 2 ) by defining edge supervised loss function. To this end, the output feature maps from the initial stage are firstly resized to the size H \u00d7 W of the original image by using bilinear interpolation up-sampling. Then the obtained large feature maps of each layer in ESM are reduced to a feature map by using 1 \u00d7 1 convolution operation. Finally each pixel value is converted to a probability by using Sigmoid function \u03c3(\u00b7) (shown in Figure 3 (a)), and an edge prediction image with H \u00d7 W is obtained. Accordingly, the edge supervised loss function is given based on Dice coefficient as follows.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 18,
                    "end": 21,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 728,
                    "end": 736,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1168,
                    "end": 1176,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Edge Supervised Module (ESM)"
        },
        {
            "text": "where S i edge is the edge prediction image obtained by using bilinear interpolation up-sampling in the i th stage. G edge is the corresponding Ground Truth (GT) of edge image, which is obtained by generating edge GT from the segmentation mask. l is the number of stages used for edge supervised in the ESM. \u03b6 i (i = 1, . . . , l) is the weight coefficient of the i th stage. By using skip connections and AFM, the edge features in the high-level feature maps can also be strengthened. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Edge Supervised Module (ESM)"
        },
        {
            "text": "For the multi-scale object segmentation, the multi-level loss function is used to build receptive fields of different sizes for different layers in the network. For example, FPN [24] uses multi-level auxiliary loss to detect objects at different scales, and it is a great breakthrough in multi-scale object detection task. Inspired by this, we develop an ASSM based on the similar strategy in our network. Specifically, the semantic information is gradually strengthened along with the down-sampling process in the encoder, and the high-level feature map has rich semantics but low spatial resolution without detailed information. Different layers contain different level semantic features according to the feature hierarchy of the contracting path. Thereby we can define an auxiliary semantic loss function to reduce the semantic gaps between high-level and low-level feature maps in the later stage (i.e.,S 3 \u223c S 5 ) of the encoder. Eventually, low-level semantic features can be strengthened by using multi-scale skip connections and AFM, and it can also reduce the background noise in the low-level feature maps.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Auxiliary Semantic Supervised Module (ASSM)"
        },
        {
            "text": "Similar to the above steps in ESM, we can obtain one coarse segmented image with the size of H \u00d7 W and the probability of each pixel through a series of operations, such as bilinear interpolation, 1 \u00d7 1 convolution, and Sigmoid function \u03c3(\u00b7) (shown in Figure 3(b) ). Then the auxiliary semantic loss function is defined based on Dice coefficient as follows.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 252,
                    "end": 263,
                    "text": "Figure 3(b)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Auxiliary Semantic Supervised Module (ASSM)"
        },
        {
            "text": "where S i mask and G mask are the obtained coarse segmented image in the i th stage of the Encoder and the Ground Truth (GT) of segmentation mask, respectively. \u03c9 i (i = l + 1, . . . , 5) is the weight coefficient of the i th stage.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Auxiliary Semantic Supervised Module (ASSM)"
        },
        {
            "text": "As mentioned above, high-level features are very efficient in semantic segmentation tasks. However, the high-level feature maps easily lead to inferior results for small or thin objects owing to the fact that the operations of convolution and pooling can cause the detailed information missing, thereby high-level feature maps have coarse resolution. To compensate the lost detailed information in high-level representations, it is necessary to import low level features. However, the full-scale skip connections can only incorporate low-level details with high-level semantics from feature maps in different scales of the same level, and the semantic gaps existing among various levels hampers the effectiveness of the semantic segmentation. Thus we propose the AFM to fuse multi-scale feature maps of different levels by using an attention mechanism to strengthen and supplement the lost detailed information in high-level representations. Gated Fully Fusion(GFF) [31] can selectively fuse features from multiple levels using gates in a fully connected way, and add weights to each spatial position by using skip connection. Inspired by this idea, an attention mechanism is incorporated into the AFM by aggregating different level features, aiming at reducing the semantic gaps between low-level features and high-level features. The corresponding attention mechanism is illustrated in Figure 4 . In general, we can directly obtain the segmentation maps from the top feature map X 1 (\u2208 R C\u00d7H\u00d7W , where c, h and w are the channel number, height and width, respectively) of the expansive path in the standard U-Net. The X 1 has high spatial resolution because the outputs need to be with the same resolution as the input image, but actually, multiple downsampling and up-sampling operations make the deep network cause mistake and loss in the detailed information. As well as strengthening the top feature map X 1 , therefore, we can aggregate feature maps of other levels (i.e., X 2 \u223c X 5 ) to supplement the lost detailed information caused by the filters or pooling operations. More precisely, we can obtain a confidence map P 1 (\u2208 R C\u00d7H\u00d7W ) through the attention block (AB) of the top feature map X 1 . The points with high confidence have a greater possibility to retain the original feature map values, and vice versa. Similarly, the lost detailed information is represented by the confidence map 1 \u2212 P 1 , in which the higher the value, the less object information it contains. Thus, we can strengthen the top feature map X 1 through the dot product between the confidence map P 1 and X 1 , and can supplement the lost detailed information by using dot product between the confidence map 1 \u2212 P 1 and the sum of other feature maps. The procedure of the attention block is illustrated in Figure 5 , and the final prediction result S P can be defined as follows.",
            "cite_spans": [
                {
                    "start": 966,
                    "end": 970,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 1388,
                    "end": 1396,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 2793,
                    "end": 2801,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "where Y i is the output by using the attention block to process the corresponding X i . While X i is firstly up-sampled to the same size with the input image by bilinear interpolation. Then Y i can be obtained by processing the up-sampling intermediate result X u i based on the attention block, and it is defined as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "where \u03a6 A (\u00b7) is the attention function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "The specific process is as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "1. Each up-sampling feature map (X i ) is processed through an attention block.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "2. After an 1 \u00d7 1 convolution operation, the channels are reduced to 64, and we can obtain the i th level feature maps.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "3. Then the resolution is resized to H \u00d7 W by using bilinear interpolation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "4. After the operation of a convolution and Sigmoid function \u03c3(\u00b7), we can obtain the confidence output Y i by using a dot product Y i = P i \u00b7 X u i . Note that the top feature map X 1 is selected as the main prediction, while other confidence output only as the supplement of Y 1 = \u03a6 A (X 1 ). When P 1 is small, it means that the corresponding confidence is low, and thereby we can compensate the lost information by doing a dot product between (1 \u2212 P 1 ) and the sum of the confidence outputs of other layer feature maps X i (i = 2, . . . , 5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "5. Finally, the final prediction result S p is obtained by summing the residuals of X 1 . The specific process is shown in Algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "The loss function for fusion is defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "where G represents the ground truth of COVID-19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Fusion Module (AFM)"
        },
        {
            "text": "Require: Feature map while up-sampling X i (i\u2208[1,. . . ,5]) Ensure: Prediction S p 1: Adopt 1 \u00d7 1 convolution on X i to change its channel number to 64 2: Resize the above obtained feature maps to the original image size of H \u00d7 W by using up-sample, and obtain X u i 3: Adjust X u i to one channel by using 3 \u00d7 3 convolution 4: Generate the confidence map P i by adopting Sigmoid function \u03c3(\u00b7) 5: Obtain Y i by doing a dot product between X u i and P i in each channel, and perform the sums 5 i=2 Y i 6: Do a dot product between (1 \u2212 P 1 ) and 5 i=2 Y i , and obtain (1 \u2212 P 1 ) 5 i=2 Y i 7: Obtain the prediction S p by calculating the summation of X 1 , Y 1 and (1 \u2212 P 1 ) 5 i=2 Y i 8: return S p L total = \u03b8L edge + \u03b2L semantic + L f usion (6) where \u03b8 and \u03b2 are weight coefficients.",
            "cite_spans": [
                {
                    "start": 742,
                    "end": 745,
                    "text": "(6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 1 Fusion Algorithm"
        },
        {
            "text": "Considering the fact that there would be negative values in the category imbalance case when using the crossentropy loss function. Therefore, we select Dice loss to supervise the predictions and labels in our experiments. To achieve deep fusions and supervisions for the features of different level, the overall loss function integrates ESM, ASSM and AFM, given as Eq. 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1 Fusion Algorithm"
        },
        {
            "text": "We collect the COVID-19 segmentation dataset from two sources. One is from [32] , including more than 900 CT images, among them about 400 slices with infections. Another is from [33] , and it contains 3D CT images of 20 patients, and we can obtain 3686 images by converting from 3D volumes into 2D slices. Due to the small datasets, the two sources are put together in a total of 4449 2D slices, among them 4000 slices for training sets and 449 slices for test sets, respectively. The GT contains four categories: 0 \u223c 3 represent background, ground glass, consolidation and plural effect, respectively. Owing to the imbalance of infection categories in the dataset, for example, only few slices contain plural effect infection, we take all types of infection as one type. Considering the limitation of GPU memory, we resize the image resolution of 512 \u00d7 512 to 256 \u00d7 256 by bilinear interpolation, then Z-score is used for data normalization. Besides, to further verify the effectiveness and generalization ability of the proposed method, we select three additional public COVID-19 datasets for testing and comparison, including MosMedData [42] , UESTC-COVID-19 [41] and COVID-ChestCT [43] . MosMedData is a dataset of 100 axial CT images from more than 40 patients with COVID-19, including 829 slices with 512x512 size (see [42] for details), and UESTC-COVID-19 contains CT scans (3D volumes) of 50 patients diagnosized with COVID-19 from 10 different hospitals(see [41] for details). While COVID-ChestCT is a small dataset, and it contains 20 CT scans of patients diagnosed with COVID-19 as well as segmentations of lungs and infections made by experts (see [43] for details). We select ResUNet as the backbone of the proposed network, in which the down-sampling of U-Net is replaced with ResNet. To verify the effectiveness of the proposed scheme, we use a series of popular segmentation models for comparison in the medical image segmentation area, such as U-Net [11] , UNet++ [9] , and Attention U-Net [34] , and we compare our methods with two cutting-edge models from the semantic segmentation: DeepLabV3+ [18] and PSPNet [17] .",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 178,
                    "end": 182,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1660,
                    "end": 1664,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1967,
                    "end": 1971,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1981,
                    "end": 1984,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 2007,
                    "end": 2011,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 2113,
                    "end": 2117,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 2129,
                    "end": 2133,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Datasets and Baselines"
        },
        {
            "text": "We adopt three metrics to evaluate our methods, such as Dice similarity coefficient, Sensitivity (Sens.), Precision (Prec.). Besides, we also introduce three golden metrics to verify the detection and segmentation performance from the object detection field, such as Structure Measure [35] , Enhance-alignment Measure [36] , and Mean Absolute Error. In our evaluation, we select S p as the final output prediction, and measure the similarity/dissimilarity between S p and ground-truth G, which can be formulated as follows. \u2022 Dice similarity coefficient: it is used to measure the proportion of intersection between S p and G, which is defined as follows.",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 289,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 318,
                    "end": 322,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "\u2022 Structure Measure (S \u03b1 ): it is used to measure the structural similarity between a prediction S p and ground-truth G, which is more consistent with the human visual system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "where S o and S r are the object-aware similarity and region-aware similarity, respectively. \u03b1 is a balance factor between S o and S r . We report S \u03b1 using the default setting (\u03b1 = 0.5) suggested in the original paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "\u2022 Sensitivity (Sens.): it is used to measure the percentage of positive samples in the total number of patients, or the probability of no missed diagnosis. The formulation is given as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "\u2022 Precision (P rec.): it is used to measure the percentage of samples with negative test in the total number of healthy people, or the probability of not misdiagnosing. The formulation is given as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "\u2022 Enhanced-alignment Measure (E mean \u03c6 ): it is a recently proposed metric for evaluating both local and global similarity between two binary maps. The formulation is given as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "where w and h are the width and height of ground-truth G, and (x, y) denotes the coordinate of each pixel in G. Symbol \u03c6 is the enhanced alignment matrix. We obtain a set of E \u03c6 by converting the prediction S p into a binary mask with a threshold from 0 to 255. In our experiments, we report the mean of E \u03be computed from all the thresholds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "\u2022 Mean Absolute Error (M AE): it is used to measure the pixel-wise error between S p and G, which is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "For the hyper-parameters in the experiments is given in Table 1 by try-and-error, respectively. Note that the learning rate is initially selected as 1e-4, then is reduced by a factor of 0.5 when the test loss is not improved within 25 epoch. Early stopping is used to avoid over-fitting. All experiments are conducted on a desktop computer with an E3-1230 v5 3.40GHz 8-core processor, and with a GeForce GTX 1070 graphics card. A GPU implementation accelerates the forward propagation and back propagation routines by using the Adam optimizer under the Pytorch framework. Each experiment is run three times, then its average and standard deviation \u00b1 are obtained. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 56,
                    "end": 63,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Evaluation Metrics and Experimental Settings"
        },
        {
            "text": "A series of comparison experiments are implemented on our dataset, and the results are shown in Table 2 . From Table 2 , the proposed method can achieve the best performances among these methods in Dice, Sens. and P rec.. Thereinto, our method has improved by around 4.4% and 1.44% in the main metric-Dice coefficient compared with U-Net [11] and Inf-Net [6] , respectively. In particular, UNet++ [9] and Attention U-Net [34] represent the best U-Net-based methods in the medical image processing area, while Inf-Net [6] , CE-Net [38] and CPFNet [40] are the newest and best methods for the segmentation of medical images. It suggests that the proposed scheme is effective and competitive, and can effectively fuse the multi-scale and multi-level features to accurately achieve the COVID-19 infection segmentation.",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 342,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 355,
                    "end": 358,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 397,
                    "end": 400,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 421,
                    "end": 425,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 517,
                    "end": 520,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 530,
                    "end": 534,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 546,
                    "end": 550,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 96,
                    "end": 103,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 111,
                    "end": 118,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Quantitative results"
        },
        {
            "text": "Besides, we further analyze the influence of edge supervision in different levels on segmentation performance by adding or reducing level edge supervision in the low-level features. To facilitate the analysis, ResUNet with Table 2 : Comparisons between different networks on our dataset. Bold black text and blue text represent the first and second best results, respectively. Co-supervision and Fusion Model (ResUNet_C i F) represents the first i levels (i.e., S 1 , . . . , S i ) in the low level to use ESM, while the rest (i.e., S i+1 , . . . , S n ) adopt ASSM in the Co-supervision, where n is the number of down-sampling (n = 5 here). The results is illustrated in Table 3 , and it is obvious that Dice coefficient firstly rises and then declines along the first level number i from 1 to 5. When i = 2 (i.e., ResUNet_C 2 F), the proposed method can obtain the best segmentation performance. It means that the features of low-level boundary and high-level semantic can both be strengthened as the first level number i increases and reduces, respectively. When i = 2, there is a trade-off between the number of low-level and high-level (i.e., the use of context and localization accuracy), consequently ResUNet_C 2 F can surpass other ResUNet_C i F in most metrics, such as Dice, M AE, E \u03c6 and S \u03b1 . More precisely, the proposed ESM and ASSM can incorporate low-level details with high-level semantics from feature maps in different levels by using AFM.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 223,
                    "end": 230,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 672,
                    "end": 679,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Quantitative results"
        },
        {
            "text": "To further demonstrate the effectiveness of the proposed scheme, we visualize the prediction results of different networks. As shown in Figure 6 , our method can remarkably outperform the baseline methods in the lung infection segmentation. Specifically, our segmentation results have much less mis-segmented tissues, while there are a lot of lossing and improper segmentation in the baseline U-Net and other methods. For the infection edge marked with a red box, for instance, our method can obtain a complete edge, and it is much closer to the real label in edge detail, which benefits from the more detailed edge information provided by the proposed ESM. Besides, from the regions marked by the blue box, our method can avoid over-segmentation, under-segmentation and incorrect segmentation efficiently. Especially in the 4 th rows, only our method and Deeplabv3+ can correctly detect the small infection (marked the blue box). It can be also observed obviously that our method is better than Deeplabv3+ in the edge details of large targets (marked the red box) because our method can provide different sizes of receptive fields and have good segmentation performance for different scale objects.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 144,
                    "text": "Figure 6",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Qualitative Results"
        },
        {
            "text": "Along the down-sampling process in U-Net, edge feature information becomes less and less, while semantic one becomes richer and richer. For further verification, we visualize the feature maps of different levels (i.e., from S 1 to S 5 ) in ResUNet_C 5 F. As shown in Figure 7 , the feature maps of low-level output (S 1 and S 2 ) contain more details, and the feature map in S 3 is the closest to the edge GT. With the deepening of down-sampling, edge feature information becomes less obvious. In the back propagation, we can extract more semantic information from the feature maps of high-level, as shown in S 5 . It demonstrates that our ESM in low-level and ASSM in hige-level are very efficient to deal with a such difficult segmentation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 267,
                    "end": 275,
                    "text": "Figure 7",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Qualitative Results"
        },
        {
            "text": "To further analyse and test the validity of the proposed modules, a series of comparison experiments are conducted on our dataset by using various combinations among ESM, ASSM and AFM based on the baseline ResUNet. The experimental results are shown in the third row of Table 4 , and each module can improve independently the Dice coefficient of infection segmentation. Thereinto, compared with the baseline ResUNet without any other modules, ASSM can obtain independently the greatest performance improvements, followed by AFM. While for various combinations between ESM, ASSM and AFM, they can also outperform their separate modules, and the combination of ASSM and AFM can obtain slightly better performance than that of ESM and AFM. Finally, the combination of the three modules can obtain the best performance, the reason is that the integration can take full advantage of them and obtain the optimal segmentation effect. Our network can be generalized for other segmentation applications due to the effectiveness of its architecture.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 270,
                    "end": 277,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Ablation Experiments"
        },
        {
            "text": "To test the effects of the proposed modules in the decoder, ESM and ASSM are applied separately or jointly in the up-sampling path. For convenience, * indicates the corresponding modules and stages in the up-sampling path (shown in Figure 2 ). Owing the symmetric structure between the encoder and decoder, ESM and ASSM are symmetrically placed in the low level (i.e., S * 1 to S * 2 ) and high level (i.e., S * 3 to S * 5 ) of the up-sampling path, respectively. The experimental results are shown in the fourth row of Table 4 . Compared with the baseline method, the Dice performance can be improved in certain extent when these modules are separately or jointly adopted in up-sampling path, particularly the combination of the three modules can obtain the second best segmentation performance. However, the obtain performance in the up-sampling path is slightly worse than that of the corresponding down-sampling path in general. It means that the proposed Co-supervision scheme can both guide the network learning the features of edges and semantics in the down-sampling and up-sampling paths, but the effect would be more appreciable when the supervision modules is applied in the down-sampling path. The reason is that the levels of the down-sampling path contain richer primitive feature information than those of the up-sampling path owing to the encoder close to the original input data, while the edge and semantic information exist more or less some loss and noise when reconstructing a higher resolution layers by using bilinear interpolation up-sampling. Accordingly, the supervision in the levels of the down-sampling path is more stronger than that of the up-sampling path.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 232,
                    "end": 240,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 520,
                    "end": 527,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Ablation Experiments"
        },
        {
            "text": "However, interestingly, the segmentation performance is even decreased compared with the baseline method when the proposed Co-supervision scheme are simultaneously applied in the down-sampling and up-sampling paths, and the fourth row of Table 4 shows the results. Except the combination between ESM, ESM * and AFM, all combinations between the down-sampling and up-sampling paths can obtain poorer segmentation performance than the baseline method. While the combination between ESM, ESM * and AFM can increase by about 1% over the baseline method. The most probable cause is the conflict and interference of the Co-supervision between the down-sampling and upsampling paths. For example, the down-sampling path (i.e., encoder) is used to encode the input image into feature representations at multiple different levels, thereby capturing the context of the image like edge detail information. While the up-sampling path (i.e., decoder) is to semantically project the discriminative features (lower resolution) learned by the encoder onto the pixel space (higher resolution) to get a precise localization. Correspondingly, the loss function is to put more emphasis on edge details in the encoder path, while to highlight localization information for the decoder path. But all the feature maps of the decoder come from the encoder by concatenating and up-sampling, which results in the conflict and interference between the encoder and decoder when the Co-supervision modules are simultaneously applied in the two paths. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 238,
                    "end": 245,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Ablation Experiments"
        },
        {
            "text": "Multilevel feature fusion means different level of feature maps are integrated together to enrich the feature information, and traditional fusion approaches usually use feature addition or concatenation. An addition process is to add multiple feature maps to be one, which means that the amount of information under the characteristics of the description image is increased. While a concatenation is a combination of the number of channels, which means that the features describing the image itself are increased, but the information under each feature is not increased. To further verify the advantage of the proposed AFM, a series of comparison experiments are carried out by only using different fusion approaches, and the segmentation results are shown in Table 5 . It can be seen that the proposed AFM can surpass the other two methods in all metrics except S \u03b1 (%). The reason is that all feature maps are evenly fused according to the same importance in the adding or concatenating process. However, it is obviously unreasonable because there are great differences between different levels in feature representations, and it is not sufficient to adaptively compensate low level finer details to high level semantic features only by simple adding or concatenating operation. Meanwhile, the concatenation operation can reduce the weight of the feature maps with poor semantics in the subsequent features in the convolution layer, while retaining rich semantic features in the channel. Whereas the addition operation can weaken the discrimination of features due to the simple pixel-wise summation for the feature maps. Therefore, the concatenation fusion method can surpass the addition operation. Figure 8 illustrates the visual results of the fusion process by utilizing an attention mechanism. Y 1 is only processed by the attention block (AB), thereby it is the nearest output to the segmentation prediction of the baseline. While S p is the segmentation results by fusing multiple level feature maps, which would achieve the goal of both high resolution and rich semantics by combining the complementary strengths of multiple level feature maps. It is obvious that the S p is more complete than the P 1 , and its lost information is lesser than that of the P 1 . The obtain confidence P 1 attaches importance to the P 1 to ensure the most information retained. As a complement to the P 1 , whereas, the confidence map 1 \u2212 P 1 pays attention to the lost detailed information, and it can exploit sufficient spatial and semantic features to supplement the lost detailed information by fusing different levels. Thus the proposed methods can overcome the under-segmentation problem of the baseline, and retain multi-scale contextual information from multiple different levels. Tables 6-8 . For the MosMedData dataset, our method is slightly superior than Attention U-Net [34] and UNet++ [9] with Dice metric, but it can obtain 3.06% better than its nearest competitor F3Net [20] with Sensitivity (Sens.), and can achieve the best performance among these three methods with all metrics (shown in Table  6 ). In the UESTC-COVID-19 dataset, our method is slightly better than its nearest competitor with Dice and M AE metrics, and is slightly lower than its nearest competitor in Sensitivity (Sens.). Overall, our method can obtain the best comprehensive performance among these methods (shown in Table 7 ). As for the COVID-ChestCT, our method can achieve the first, first and third best performance in Sensitivity (Sens.), M AE and Dice, respectively. Compared with other methods, our method can also achieve the best overall performance (shown in Table 8 ). From the above results, our method can achieve the first three best performances for various datasets using all metrics, and has the best comprehensive performance comparing to other methods.",
            "cite_spans": [
                {
                    "start": 2876,
                    "end": 2880,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 2892,
                    "end": 2895,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 2979,
                    "end": 2983,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 760,
                    "end": 767,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 1703,
                    "end": 1711,
                    "text": "Figure 8",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 2782,
                    "end": 2792,
                    "text": "Tables 6-8",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 3100,
                    "end": 3108,
                    "text": "Table  6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 3399,
                    "end": 3406,
                    "text": "Table 7",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 3652,
                    "end": 3659,
                    "text": "Table 8",
                    "ref_id": "TABREF7"
                }
            ],
            "section": "Comparison of Fusion Methods"
        },
        {
            "text": "It is still a challenging task to accurately segment the infected lesions of COVID-19 on CT images owing to the irregular shapes with various sizes and indistinguishable boundaries between normal and infected tissues. In this paper, a novel segmentation scheme is proposed for the infection segmentation of COVID-19 on CT Images. To achieve this, we propose three modules for deep collaborative supervision and attention fusion based on ResUnet. To verify the effectiveness of the proposed scheme, a series of experiments are conducted on four COVID-19 datasets. The results show that our method can achieve the best performance for most of the datasets with metrics, such as Dice, Sensitivity(Sens.) and M AE, and has better generalization performance comparing to the existing approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The proposed technique has four advantages as follows. Firstly, it is able to capture rich spatial information in various scales through an edge supervised module, denoted as the ESM, which allows to incorporate the edge supervised information into the initial stage of down-sampling in the framework of ResUnet. As low-level layers contain richer object boundaries, they are used to define the edge supervised loss function to capture all spatial information. The main benefit of this module is to highlight low-level boundary features and provide useful fine-grained constraints to guide feature extraction in semantic segmentation tasks. Secondly, the proposed method can explore semantic information from various scale infections on COVID-19 CT images by using an auxiliary semantic supervised module (i.e., ASSM) that can integrate the appearance supervised information into the later stage of down-sampling. The main advantage of this module is to strengthen high-level semantic information during the feature extraction process. Thirdly, we propose an attention fusion module (i.e., AFM) to fuse multiple scale feature maps of different levels from the up-sampling stage to reduce the semantic gaps between high-level and low-level feature maps. The main advantage of this module is to strengthen and supplement the lost detailed information in high-level representations. Lastly, we construct a joint loss function by combining the edge supervised loss, auxiliary semantic supervised loss and fusion loss. The joint function can guide the network in learning the features of COVID-19 infections, thereby achieving a deep collaborative supervision on edges and semantics. Meanwhile, it can also act as an incentive to effectively fuse multi-scale feature maps of different levels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Although our network can get a good result in segmenting the overall infection region, it is not sufficient to estimate the severity of infected COVID-19, because finer segmentation of the different infection regions is required. In the future, we might collect a large amount of COVID-19 data, and consider further recognizing the severity of COVID-19 according to the area, size, and location of infections.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A novel coronavirus outbreak of global health concern",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "W"
                    ],
                    "last": "Horby",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "G"
                    ],
                    "last": "Hayden",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "F"
                    ],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Lancet",
            "volume": "395",
            "issn": "10223",
            "pages": "470--473",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "395",
            "issn": "",
            "pages": "497--506",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Synergistic learning of lung lobe segmentation and hierarchical multi-instance classification for automated severity assessment of COVID-19 in CT images",
            "authors": [
                {
                    "first": "Kelei",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Xingzhi",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Wen",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "Mingxia",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Zhenyu",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Yinghuan",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Feng",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Yang",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Junfeng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Dinggang",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Pattern Recognition",
            "volume": "113",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Value of quantitative analysis in lung computed tomography in patients severely ill with COVID-19",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rorat",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Jurek",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simon",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Guzi\u0144ski",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "PLoS ONE",
            "volume": "16",
            "issn": "5",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0251946"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Multi-task Deep Learning Based CT Imaging Analysis For COVID-19 Pneumonia: Classification and Segmentation",
            "authors": [
                {
                    "first": "Amine",
                    "middle": [],
                    "last": "Amyar",
                    "suffix": ""
                },
                {
                    "first": "Romain",
                    "middle": [],
                    "last": "Modzelewski",
                    "suffix": ""
                },
                {
                    "first": "Hua",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Su",
                    "middle": [],
                    "last": "Ruan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Computers in Biology and Medicine",
            "volume": "126",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Inf-Net: Automatic COVID-19 Lung Infection Segmentation from CT Scans",
            "authors": [
                {
                    "first": "D P",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "G P",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Medical Imaging (TMI)",
            "volume": "39",
            "issn": "8",
            "pages": "2626--2637",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A deep learning algorithm using CT images to screen for Corona Virus Disease (COVID-19)",
            "authors": [
                {
                    "first": "Shuai",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Jinlu",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Eur Radiol",
            "volume": "",
            "issn": "8",
            "pages": "6096--6104",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Rahman Siddiquee",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tajbakhsh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support 11045",
            "volume": "",
            "issn": "",
            "pages": "3--11",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Azzeddine Kassah Laouar, Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images",
            "authors": [
                {
                    "first": "Adel",
                    "middle": [],
                    "last": "Oulefki",
                    "suffix": ""
                },
                {
                    "first": "Thaweesak",
                    "middle": [],
                    "last": "Sos Agaian",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Trongtirakul",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Pattern Recognition",
            "volume": "114",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Convolutional Networks for Biomedical Image Segmentation, Medical Image Computing and Computer Assisted Intervention (MICCAI)",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                },
                {
                    "first": "U-Net",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "9351",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Deep Learning: Methods and Applications, Foundations and Trends in Signal Processing",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "7",
            "issn": "",
            "pages": "197--387",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "authors": [
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ilya",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1097--1105",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Deep Residual Learning for Image Recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Xiangyu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Fully Convolutional Networks for Semantic Segmentation",
            "authors": [
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "Evan",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "3431--3440",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Pyramid Scene Parsing Network",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Jianping",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Xiaojuan",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "Xiaogang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Jiaya",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "2881--2890",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Encoder decoder with atrous separable convolution for semantic image segmentation",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "European Conference on Computer Vision (ECCV)",
            "volume": "",
            "issn": "",
            "pages": "801--818",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "PsaNet: Point-wise spatial attention Network for scene parsing",
            "authors": [
                {
                    "first": "Hengshuang",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shu",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jianping",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Chen",
                    "middle": [
                        "Change"
                    ],
                    "last": "Loy",
                    "suffix": ""
                },
                {
                    "first": "Dahua",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Jiaya",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "European Conference on Computer Vision (ECCV)",
            "volume": "",
            "issn": "",
            "pages": "267--283",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "F3Net: Fusion, Feedback and Focus for Salient Object Detection, AAAI Conference on Artificial Intelligence 2020 (AAAI 2020)",
            "authors": [
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Shuhui",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Qingming",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "12321--12328",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Context encoding for semantic segmentation",
            "authors": [
                {
                    "first": "Hang",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Kristin",
                    "middle": [],
                    "last": "Dana",
                    "suffix": ""
                },
                {
                    "first": "Jianping",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Zhongyue",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Xiaogang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Ambrish",
                    "middle": [],
                    "last": "Tyagi",
                    "suffix": ""
                },
                {
                    "first": "Amit",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "7151--7160",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Stacked cross refinement Network for edge-aware salient object detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "7264--7273",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A Generic Edge-aTtention Guidance Network for Medical Image Segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Et-Net",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Medical Image Computing and Computer Assisted Intervention (MICCAI)",
            "volume": "11764",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Feature Pyramid Networks for Object Detection",
            "authors": [
                {
                    "first": "Tsung-Yi",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [
                        "B"
                    ],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Bharath",
                    "middle": [],
                    "last": "Hariharan",
                    "suffix": ""
                },
                {
                    "first": "Serge",
                    "middle": [
                        "J"
                    ],
                    "last": "Belongie",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "936--944",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "authors": [
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kaiming",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "91--99",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Mask r-cnn",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Georgia",
                    "middle": [],
                    "last": "Gkioxari",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE international conference on computer vision (ICCV",
            "volume": "",
            "issn": "",
            "pages": "2961--2969",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Detection and Recognition for Life State of Cell Cancer Using Two-Stage Cascade CNNs",
            "authors": [
                {
                    "first": "Haigen",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Qiu",
                    "middle": [],
                    "last": "Guan",
                    "suffix": ""
                },
                {
                    "first": "Shengyong",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Zhiwei",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "Lin",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
            "volume": "2020",
            "issn": "3",
            "pages": "887--898",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Dual Attention Network for Scene Segmentation",
            "authors": [
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "Jing",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Haijie",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "Yong",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Yongjun",
                    "middle": [],
                    "last": "Bao",
                    "suffix": ""
                },
                {
                    "first": "Zhiwei",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Hanqing",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "3146--3154",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "CCNet: Criss-Cross Attention for Semantic Segmentation, International Conference on Computer Vision (ICCV)",
            "authors": [
                {
                    "first": "Zilong",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Xinggang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Lichao",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Chang",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Yunchao",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Wenyu",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "603--612",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Non-Local Neural Networks, Conference on Computer Vision and Pattern Recognition (CVPR)",
            "authors": [
                {
                    "first": "Xiaolong",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "Abhinav",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "7794--7803",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "GFF: Gated Fully Fusion for Semantic Segmentation, Association for the Advance of Artificial Intelligence (AAAI)",
            "authors": [
                {
                    "first": "Xiangtai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Houlong",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Yunhai",
                    "middle": [],
                    "last": "Tong",
                    "suffix": ""
                },
                {
                    "first": "Kuiyuan",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "COVID-19 CT segmentation dataset",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2020--2026",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "COVID-19 CT segmentation dataset",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2020--2026",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Attention U-Net: Learning Where to Look for the Pancreas, International Conference on Medical Imaging with Deep Learning (MIDL)",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Oktay",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schlemper",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Structure-measure:A new way to evaluate foreground maps",
            "authors": [
                {
                    "first": "Deng-Ping",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Ming-Ming",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Yun",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE International Conference on Computer Vision (ICCV",
            "volume": "",
            "issn": "",
            "pages": "4548--4557",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Enhanced-alignment measure for binary foreground map evaluation",
            "authors": [
                {
                    "first": "D.-P",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Joint Conference on Artificial Intelligence (IJCAI)",
            "volume": "",
            "issn": "",
            "pages": "698--704",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "An adaptive learning method of anchor shape priors for biological cells detection and segmentation",
            "authors": [
                {
                    "first": "Haigen",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Aizhu",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Qianwei",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Qiu",
                    "middle": [],
                    "last": "Guan",
                    "suffix": ""
                },
                {
                    "first": "Xiaoxin",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Qi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Computer Methods and Programs in Biomedicine",
            "volume": "208",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "CE-Net: Context Encoder Network for 2D Medical Image Segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Medical Imaging",
            "volume": "38",
            "issn": "10",
            "pages": "2281--2292",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "ACFNet: Attentional Class Feature Network for Semantic Segmentation",
            "authors": [
                {
                    "first": "Fan",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Yanqin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Zhihang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Zhibin",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "Jingtuo",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Feifei",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Junyu",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Errui",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE/CVF International Conference on Computer Vision, ICCV2019",
            "volume": "",
            "issn": "",
            "pages": "6797--6806",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Context pyramid fusion network for medical image segmentation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans Med Imaging",
            "volume": "39",
            "issn": "",
            "pages": "3008--3018",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "A Noise-robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions from CT Images",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ruan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Transactions on Medical Imaging",
            "volume": "2020",
            "issn": "8",
            "pages": "2653--2663",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "MosMedData: data set of 1110 chest CT scans performed during the COVID-19 epidemic",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Morozov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Andreychenko",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "A"
                    ],
                    "last": "Blokhin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Digital Diagnostics",
            "volume": "2020",
            "issn": "1",
            "pages": "49--59",
            "other_ids": {
                "DOI": [
                    "10.17816/DD46826"
                ]
            }
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "COVID-19 Image Data Collection: Prospective Predictions Are the Future",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Morrison",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Dao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.11988"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An illustration of challenging task for identification the infected lesions (contours in red) of COVID-19 on CT images. (a) The infections have various scales and shapes.(b)There is no obvious difference between normal and infected tissues.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "An Illustration of the overall network architecture. The proposed architecture comprises of ASSM, ESM and AFM based on encoder-decoder structure. (1) ESM is used to further highlight the low-level features in the initial shallow layers of the encoder, and it can capture more detailed information like object boundaries. (2) While ASSM is employed to strengthen high-level semantic information by integrating object mask supervised information into the later stages of the encoder. (3) Finally, AFM is utilized to fuse multi-scale feature maps of different levels in the decoder.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An illustration of ESM and ASSM. Firstly, the low resolution feature maps from the stage S i are resized to the same size H \u00d7 W with the input image by using bilinear interpolation up-sampling. Then all high resolution feature maps are reduced to a feature map by using 1 \u00d7 1 convolutions. Finally each pixel value of the obtained feature map is converted to a probability by using Sigmoid function \u03c3(\u00b7), and the prediction image of the S i stage is obtained. (a) ESM: the edge supervision is achieved by comparing between the obtained edge prediction image S i edge and the corresponding edge Ground Truth (GT) G edge based on Eq.(1). (b)ASSM: the auxiliary semantic supervision is achieved by comparing between the obtained coarse segmented image S i mask and the corresponding Ground Truth (GT) of segmentation mask G mask based on Eq.(2).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "An illustration of the attention mechanism. X u i represents the up-sampling intermediate result by bilinear interpolation for the feature map X i , and its 2D size is the same size with the input image.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The procedure of the attention block. The color bar represents the trends of confidence values, and the red and blue denote 1 and 0, respectively.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Visual qualitative comparison of lung infection segmentation results among U-Net, PSPNet, DeepLabv3+, Inf-Net and the proposed method. Column 1: the original CT image; Column 2: U-Net; Column 3: PSPNet; Column 4: DeepLabv3+; Column 5: Inf-Net; Column 6: our method; Column 7: the corresponding ground truth (GT).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Visualization of each stage supervised by ESM. Column 1: the original CT image; Columns 2 to 6: S 1 to S 5 ; Column 7: the corresponding edge ground truth (GT).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Visual results of the fusion process based on the proposed AFM. Column 1: the original CT image; Column 2: the obtained confidence map P 1 ; Column 3: the confidence map 1 \u2212 P 1 of the lost detailed information; Column 4: the major result Y 1 from the top feature map x 1 through the attention block (AB); Column 5: the final prediction result S p ; Column 6: the corresponding ground truth (GT).",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Hyperparameter setting",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The results of different numbers of edge supervised on our dataset. Bold black text and blue text represent the first and second best results, respectively.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Ablation Experiments on our dataset. Bold black text and blue text represent the first and second best results, respectively.Baseline ESM ASSM ASSM * ESM * AFM Dice(%) \u2191",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "The results of different fusion methods on our dataset",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Performance comparisons between different methods on MosMedData. Bold black text, blue text and green text represent the first, second and third best results, respectively.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Performance comparisons between different methods on UESTC-COVID-19. Bold black text, blue text and green text represent the first, second and third best results, respectively. Ours) 85.52\u00b10.081 79.46\u00b12.286 0.47\u00b10.006 4.6 Comparisons on other COVID-19 Datasets To further verify the effectiveness and generalization ability, a series of comparison experiments are conducted on MosMedData [53], UESTC-COVID-19[52] and COVID-ChestCT [54], respectively. We select three important metrics for the evaluation of the COVID-19 lung infection segmentation, including Dice, Sens. and M AE. The results are shown in",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Performance comparisons between different methods on COVID-ChestCT. Bold black text, blue text and green text represent the first, second and third best results, respectively. Ours) 72.81\u00b10.148 83.89\u00b11.358 0.74\u00b10.017",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors would like to express their appreciation to the referees for their helpful comments and suggestions. This ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement"
        }
    ]
}